{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTerReads: Proof of concept notebook\n",
    "\n",
    "This notebook outlines how the BERTerReads app works. The process is simple:\n",
    "\n",
    "1. A GoodReads book URL is provided as input\n",
    "1. The first page of the book's reviews are scraped live on the spot\n",
    "1. The reviews are divided into their individual sentences\n",
    "1. Each sentence is transformed into a 768-dimensional vector with DistilBERT\n",
    "1. The set of vectors is run through a K-means clustering algorithm, dividing the sentences into 3 clusters\n",
    "1. The vector closest to each cluster centre is identified\n",
    "1. The sentences corresponding to these 3 vectors are displayed back to the user\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DistilBERT model\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Retrieve URL from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input GoodReads book URL: https://www.goodreads.com/book/show/51791252-the-vanishing-half\n"
     ]
    }
   ],
   "source": [
    "url = input('Input GoodReads book URL:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrape reviews from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(url):\n",
    "    '''\n",
    "    Function to scrape all the reviews from the first page of a GoodReads book URL\n",
    "    '''\n",
    "\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, features='html.parser')\n",
    "\n",
    "    reviews_src = soup.find_all('div', class_='reviewText stacked')\n",
    "\n",
    "    reviews = []\n",
    "\n",
    "    for review in reviews_src:\n",
    "\n",
    "        reviews.append(review.text)\n",
    "\n",
    "    df = pd.DataFrame(reviews, columns=['review'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = get_reviews(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Divide reviews into individual sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reviews(df):\n",
    "    '''\n",
    "    Function to clean review text and divide into individual sentences\n",
    "    '''\n",
    "\n",
    "    # Define spoiler marker & \"...more\" strings, and remove from all reviews\n",
    "    spoiler_str_gr = '                    This review has been hidden because it contains spoilers. To view it,\\n                    click here.\\n\\n\\n'\n",
    "    more_str = '\\n...more\\n\\n'\n",
    "    df['review'] = df['review'].str.replace(spoiler_str_gr, '')\n",
    "    df['review'] = df['review'].str.replace(more_str, '')\n",
    "\n",
    "    # Scraped reviews from GoodReads typically repeat the first ~500 characters\n",
    "    # The following loop removes these repeated characters\n",
    "\n",
    "    # Loop through each row in dataframe\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # Save review and review's first ~250 characters to variables\n",
    "        review = df.iloc[i]['review']\n",
    "        review_start = review[2:250]\n",
    "\n",
    "        # Loop through all of review's subsequent character strings\n",
    "        for j in range(3, len(review)):\n",
    "\n",
    "            # Check if string starts with same sequence as review start\n",
    "            if review[j:].startswith(review_start):\n",
    "                # If so, chop off all previous characters from review\n",
    "                df.at[i, 'review'] = review[j:]\n",
    "\n",
    "    # Replace all new line characters\n",
    "    df['review'] = df['review'].str.replace('\\n', ' ')\n",
    "\n",
    "    # Append space to all sentence end characters\n",
    "    df['review'] = df['review'].str.replace('.', '. ').replace('!', '! ').replace('?', '? ')\n",
    "\n",
    "    # Initialize dataframe to store review sentences, and counter\n",
    "    sentences_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each review\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # Save row and review to variables\n",
    "        row = df.iloc[i]\n",
    "        review = row.loc['review']\n",
    "\n",
    "        # Tokenize review into sentences\n",
    "        sentences = sent_tokenize(review)\n",
    "\n",
    "        # Loop through each sentence in list of tokenized sentences\n",
    "        for sentence in sentences:\n",
    "            # Add row for sentence to sentences dataframe\n",
    "            new_row = row.copy()\n",
    "            new_row.at['review'] = sentence\n",
    "            sentences_df = sentences_df.append(new_row, ignore_index=True)\n",
    "\n",
    "    sentences_df.rename(columns={'review':'sentence'}, inplace=True)\n",
    "\n",
    "    lower_thresh = 5\n",
    "    upper_thresh = 50\n",
    "\n",
    "    # Remove whitespaces at the start and end of sentences\n",
    "    sentences_df['sentence'] = sentences_df['sentence'].str.strip()\n",
    "\n",
    "    # Create list of sentence lengths\n",
    "    sentence_lengths = sentences_df['sentence'].str.split(' ').map(len)\n",
    "\n",
    "    num_short = (sentence_lengths <= lower_thresh).sum()\n",
    "    num_long = (sentence_lengths >= upper_thresh).sum()\n",
    "    num_sents = num_short + num_long\n",
    "\n",
    "    # Filter sentences\n",
    "    sentences_df = sentences_df[\n",
    "        (sentence_lengths > lower_thresh) & (sentence_lengths < upper_thresh)]\n",
    "\n",
    "    sentences_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return sentences_df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = clean_reviews(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transform each sentence into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cluster sentences and print sentences closest to each cluster centre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opinions(sentences, sentence_vectors, k=3, n=1):\n",
    "    '''\n",
    "    Function to extract the n most representative sentences from k clusters, with density scores\n",
    "    '''\n",
    "    \n",
    "    # Instantiate the model\n",
    "    kmeans_model = KMeans(n_clusters=k, random_state=24)\n",
    "\n",
    "    # Fit the model\n",
    "    kmeans_model.fit(sentence_vectors);\n",
    "    \n",
    "    # Set the number of cluster centre points to look at when calculating density score\n",
    "    centre_points = int(len(sentences) * 0.02)\n",
    "    \n",
    "    # Initialize list to store mean inner product value for each cluster\n",
    "    cluster_density_scores = []\n",
    "    \n",
    "    # Initialize dataframe to store cluster centre sentences\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Loop through number of clusters\n",
    "    for i in range(k):\n",
    "\n",
    "        # Define cluster centre\n",
    "        centre = kmeans_model.cluster_centers_[i]\n",
    "\n",
    "        # Calculate inner product of cluster centre and sentence vectors\n",
    "        ips = np.inner(centre, sentence_vectors)\n",
    "\n",
    "        # Find the sentences with the highest inner products\n",
    "        top_index = pd.Series(ips).nlargest(n).index\n",
    "        top_sentence = sentences[top_index].iloc[0]\n",
    "        \n",
    "        centre_ips = pd.Series(ips).nlargest(centre_points)\n",
    "        density_score = round(np.mean(centre_ips), 5)\n",
    "        \n",
    "        # Create new row with cluster's top 10 sentences and density score\n",
    "        new_row = pd.Series([top_sentence, density_score])\n",
    "        \n",
    "        # Append new row to master dataframe\n",
    "        df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "    # Rename dataframe columns\n",
    "    df.columns = ['sentence', 'density']\n",
    "\n",
    "    # Sort dataframe by density score, from highest to lowest\n",
    "    df = df.sort_values(by='density', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        print(f\"Opinion #{i+1}: {df['sentence'][i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opinion #1: I found this to be a beautifully written and thought-provoking book.\n",
      "\n",
      "Opinion #2: While racial identity is the core of the story, there are so many other layers here with characters that the author portrays in such a way that I got a sense of who they were, even if at times they questioned their own identities.\n",
      "\n",
      "Opinion #3: Nearly broken from her sister’s choice to leave her, she never gives up hope of finding Stella until it’s nearly too late.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_opinions(sentences, sentence_vectors, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input GoodReads book URL: https://www.goodreads.com/book/show/48570454-transcendent-kingdom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraped reviews!\n",
      "Cleaned reviews!\n",
      "Embedded sentences!\n",
      "\n",
      "Opinion #1: This is definitely a book that I appreciate, respect, admire, more than I love.\n",
      "\n",
      "Opinion #2: While these experiences have affected Gifty’s relationship to her faith, and she’s somewhat embarrassed when reading her old diary entries, in which she pleads for divine intervention, as an adult Gifty finds herself craving that ardor.\n",
      "\n",
      "Opinion #3: Her brother’s addiction and her mother’s depression have irrevocably shaped Gifty, the protagonist and narrator of Transcendent Kingdom, who is now a sixth-year PhD candidate in neuroscience at Stanford.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = input('Input GoodReads book URL:')\n",
    "reviews_df = get_reviews(url)\n",
    "print('\\nScraped reviews!')\n",
    "sentences = clean_reviews(reviews_df)\n",
    "print('Cleaned reviews!')\n",
    "sentence_vectors = model.encode(sentences)\n",
    "print('Embedded sentences!\\n')\n",
    "get_opinions(sentences, sentence_vectors, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berterreads",
   "language": "python",
   "name": "berterreads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
